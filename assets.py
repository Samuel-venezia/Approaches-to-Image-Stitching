import cv2 as cv
import numpy as np
import torch
import pyiqa.models
import os
from pathlib import Path
import largestinteriorrectangle as lir
from brisque import BRISQUE

stitched_approaches = ["pipeline", "apap", "apap_sp_sg", "auto", "h_sp_sg"]  # , "open"]
margin_crop = 10


def crop_img(stitched_img):
    grey = cv.cvtColor(stitched_img, cv.COLOR_BGR2GRAY)
    _, mask = cv.threshold(grey, thresh=0, maxval=255, type=cv.THRESH_BINARY)
    # Now we have a mask, we can crop the width and height to get the largest possible image.
    contours, _ = cv.findContours(mask, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)
    x, y, w, h = cv.boundingRect(contours[0])
    return stitched_img[y:y+h, x:x+w, :]


def rectangling(stitched_img):
    _, mask = cv.threshold(cv.cvtColor(cv.GaussianBlur(stitched_img, (7, 7), cv.BORDER_DEFAULT), cv.COLOR_BGR2GRAY), thresh=0, maxval=255, type=cv.THRESH_BINARY)
    # Now we have a mask, we can crop the width and height to get the largest possible image.
    contours, _ = cv.findContours(mask, cv.RETR_TREE, cv.CHAIN_APPROX_NONE)
    # LIR: x, y, width, height
    # For real-time, the x, y, w, and h can be stored and reused rather than calculating them for each frame
    x, y, w, h = lir.lir(mask > 0, contours[0][:, 0, :])
    # mask = cv.drawContours(stitched_img, contours, -1, color=(255, 255, 255), thickness=cv.FILLED)
    # removing n pixels from the margin, n for this test is 10
    return stitched_img[y+margin_crop:y+h-margin_crop, x+margin_crop:x+w-margin_crop,:]


# For the full reference evaluation, retrieving the ground truth
def retrieve_homographies(h_name):
    matrix = open(h_name, "r").read()
    matrix = matrix.split("\n")
    np_matrix = []
    for row in matrix:
        if len(row) > 1:
            np_row = row.split(" ")
            row_new = []
            for cell in np_row:
                if cell != "":
                    row_new.append(float(cell))
            np_matrix.append(np.array(row_new))
    return np.concatenate(np_matrix).reshape(3, 3)


def find_sub_image(base_image, roi):
    if base_image.shape[2] == 3:
        base_grey = cv.cvtColor(base_image, cv.COLOR_BGR2GRAY)
    else:
        base_grey = base_image
    if roi.shape[2] == 3:
        roi_grey = cv.cvtColor(roi, cv.COLOR_BGR2GRAY)
    else:
        roi_grey = roi
    res = cv.matchTemplate(base_grey.astype(np.uint8), roi_grey.astype(np.uint8), cv.TM_CCORR_NORMED)
    min_val, max_val, min_loc, max_loc = cv.minMaxLoc(res)
    return max_loc


def formatting_images(stitched, gt, roi_image):
    stitched_c = np.copy(stitched)
    # If the stitched image is smaller than the GT, add padding to increase its size to that of the GT
    if stitched.shape[0] < gt.shape[0] or stitched.shape[1] < gt.shape[1]:
        stitched_c = np.zeros((max(stitched.shape[0], gt.shape[0]), max(stitched.shape[1], gt.shape[1]), 3),
                              dtype=np.float32)
        h_diff, w_diff = 0, 0
        if stitched.shape[0] < gt.shape[0]:
            h_diff = gt.shape[0] - stitched.shape[0]
        if stitched.shape[1] < gt.shape[1]:
            w_diff = gt.shape[1] - stitched.shape[1]
        stitched_c[h_diff:, w_diff:, :] = stitched

    # This method crops the stitched image to be the same dimension as the ground truth
    formatted_stitched = np.zeros(gt.shape, dtype="uint8")
    formatted_start_y, stitched_start_y, formatted_end_y, stitched_end_y = 0, 0, formatted_stitched.shape[0], \
                                                                           stitched_c.shape[0]
    formatted_start_x, stitched_start_x, formatted_end_x, stitched_end_x = 0, 0, formatted_stitched.shape[1], \
                                                                           stitched_c.shape[1]

    stitched_left_top = find_sub_image(stitched_c, roi_image)
    gt_left_top = find_sub_image(gt, roi_image)

    trans_y = gt_left_top[1] - stitched_left_top[1]
    trans_x = gt_left_top[0] - stitched_left_top[0]

    # If trans is positive, then move either left or down
    # If trans is negative, then move either right or up

    # stitched(roi) + trans = start = gt(roi)

    stitched_start_y = trans_y + stitched_left_top[1]
    stitched_start_x = trans_x + stitched_left_top[0]

    stitched_end_y = stitched_c.shape[0]
    stitched_end_x = stitched_c.shape[1]

    if trans_x != 0 or trans_y != 0:
        sc_start_x, sc_start_y = stitched_left_top[0] - stitched_start_x, stitched_left_top[1] - stitched_start_y
        if sc_start_x < 0:
            formatted_start_x = abs(sc_start_x)
            sc_start_x = 0
            # stitched_end_x = stitched_end_x + sc_start_x
            # sc_start_x = stitched_start_x - stitched_left_top[0]
        if sc_start_y < 0:
            formatted_start_y = abs(sc_start_y)
            sc_start_y = 0
            # stitched_end_y = stitched_end_y + sc_start_y
            # sc_start_y = stitched_start_y - stitched_left_top[1]
        stitched_crop = np.zeros(
            (max(stitched_c.shape[0], gt.shape[0]) * 2, max(stitched_c.shape[1], gt.shape[1]) * 2, 3),
            dtype="uint8")
        stitched_crop[formatted_start_y:formatted_start_y + (stitched_end_y - abs(sc_start_y)),
        formatted_start_x:formatted_start_x + (stitched_end_x - abs(sc_start_x)), :] = stitched_c[
                                                                                       abs(sc_start_y): stitched_end_y,
                                                                                       abs(sc_start_x): stitched_end_x,
                                                                                       :]
        # cv.imshow("stitched_crop", stitched_crop)
        # cv.imshow("GT", gt)
        # cv.waitKey(0)
        # stitched_left_top_ex = Feature_Extraction.find_sub_image(stitched_crop, roi_image)
        return stitched_crop[:gt.shape[0], :gt.shape[1], :]
    else:
        return stitched_c[:gt.shape[0], :gt.shape[1], :]


def vsi_eval(stitched, gt):
    stitched_tensor = torch.reshape(torch.from_numpy(np.expand_dims(stitched / 255, 0)),
                                    (1, stitched.shape[2], stitched.shape[0], stitched.shape[1])).clone().detach()
    gt_tensor = torch.reshape(torch.from_numpy(np.expand_dims(gt / 255, 0)),
                              (1, gt.shape[2], gt.shape[0], gt.shape[1])).clone().detach()
    return pyiqa.create_metric("vsi").forward(stitched_tensor, gt_tensor).numpy()[0]


def brisque_eval(stitched):
    # Evaluates a stitched image using the no reference image quality assessment "BRISQUE"
    return BRISQUE().score(stitched)


# region retrieving and generating Homography image pairs to and from a txt file.
def retrieve_adobe_files(file_name="adobe_files.txt"):
    # To get all images that can be stitched, we created a txt document that goes through each img pair within the dataset.
    # It will be structured thusly:
    # adobe_panoramas/carmel/H01to00.txt adobe_panoramas/carmel/carmel-00.png adobe_panoramas/carmel/carmel-01.png
    # A space between each of the images and the first file will be the homography needed to create the ground truth.
    img_pairs = []
    homographies = []
    with open(file_name, "r") as t:
        raw = t.readlines()
        for line in raw:
            cont = line.split(sep=" ")
            if line.endswith(".png"):
                img_pairs.append([cont[1], cont[2]])
            else:
                img_pairs.append([cont[1], cont[2][:-1]])
            homographies.append(cont[0])
    return img_pairs, homographies


def generate_h_pair_file(dataset="adobe_panoramas"):
    if not Path("dataset_list").is_dir():
        os.mkdir("dataset_list")
    file_names = os.listdir(dataset)
    for subset in file_names:
        dir_file_names = os.listdir(f"{dataset}/{subset}")
        img_names = []
        homographies = []
        for f_name in dir_file_names:
            if f_name.startswith(
                    "H"):  # Possibly change to starts with "H" as other datasets use just a file rather than a txt.
                homographies.append(f_name)
            elif f_name.endswith("png") or f_name.endswith("ppm"):
                img_names.append(f_name)
        file_img_pairs = []
        for homography in homographies:
            img_1_name, img_2_name = "", ""
            for img in img_names:
                if "hpatches" in dataset:
                    if homography[2] in img:
                        img_2_name = img
                    if homography[-1] in img:
                        img_1_name = img
                else:
                    if homography[-6:-4] in img:
                        img_1_name = img
                    if homography[-10:-8] in img:
                        img_2_name = img
            file_img_pairs.append(
                f"{dataset}/{subset}/{homography} {dataset}/{subset}/{img_1_name} {dataset}/{subset}/{img_2_name}\n")
        with open(f"dataset_list/{dataset}-{subset}.txt", 'w') as file:
            file.writelines(file_img_pairs)
# endregion
